{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 2\n",
    "\n",
    "## Definir el problema\n",
    "\n",
    "Identificar cuáles son las pólizas que realizaran un reclamo de siniestros. \n",
    "\n",
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datasets desde archivos csv\n",
    "df_insurance_data = pd.read_csv(\"insurance_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "EDA realizado en 'Desafío 1'.\n",
    "\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  INSR_BEGIN   INSR_END  POLICY_DURATION\n",
      "0 2014-07-01 2015-06-30              364\n",
      "1 2014-07-01 2015-06-30              364\n",
      "2 2014-07-01 2015-06-30              364\n",
      "3 2014-07-01 2015-06-30              364\n",
      "4 2014-07-01 2015-06-30              364\n"
     ]
    }
   ],
   "source": [
    "# Rango de fechas\n",
    "df_insurance_data['INSR_BEGIN'] = pd.to_datetime(df_insurance_data['INSR_BEGIN'], format='%d-%b-%y')\n",
    "df_insurance_data['INSR_END'] = pd.to_datetime(df_insurance_data['INSR_END'], format='%d-%b-%y')\n",
    "\n",
    "# Crear la columna de duración de la póliza en días\n",
    "df_insurance_data['POLICY_DURATION'] = (df_insurance_data['INSR_END'] - df_insurance_data['INSR_BEGIN']).dt.days\n",
    "\n",
    "print(df_insurance_data[['INSR_BEGIN', 'INSR_END', 'POLICY_DURATION']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna binaria 'CLAIM_PAID_BINARY'\n",
    "df_insurance_data['CLAIM_PAID_BINARY'] = df_insurance_data['CLAIM_PAID'].apply(lambda x: 1 if pd.notnull(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar las columnas que no voy a usar para el modelo. \n",
    "- CLAIM_PAID: representado con la variable binaria.\n",
    "- Fechas: representado con el rango de fechas. \n",
    "- POLICY_ID: e sun identificador único para cada registro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insurance_data_cleaned = df_insurance_data.drop(columns=['CLAIM_PAID', 'INSR_BEGIN', 'INSR_END', 'POLICY_ID'])\n",
    "\n",
    "# Verificar que las columnas fueron eliminadas\n",
    "# print(df_insurance_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación one-hot para convertir columnas categóricas en valores numpericos binarios\n",
    "# Necesario para modelos de ML proque no pueden procesar directamente datos categóricos.\n",
    "df_insurance_data_cleaned = pd.get_dummies(df_insurance_data_cleaned, columns=['SEX', 'INSR_TYPE', 'USAGE'], drop_first=True, dtype = int)\n",
    "\n",
    "# Verificar que las columnas fueron eliminadas\n",
    "# print(df_insurance_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 406448 entries, 0 to 406447\n",
      "Data columns (total 22 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   CUSTOMER_SENIORITY            406448 non-null  int64  \n",
      " 1   INSURED_VALUE                 406448 non-null  float64\n",
      " 2   PREMIUM                       406439 non-null  float64\n",
      " 3   VEHICLE_ID                    406448 non-null  int64  \n",
      " 4   POLICY_DURATION               406448 non-null  int64  \n",
      " 5   CLAIM_PAID_BINARY             406448 non-null  int64  \n",
      " 6   SEX_Male                      406448 non-null  int32  \n",
      " 7   INSR_TYPE_1202                406448 non-null  int32  \n",
      " 8   INSR_TYPE_1204                406448 non-null  int32  \n",
      " 9   USAGE_Agricultural Own Farm   406448 non-null  int32  \n",
      " 10  USAGE_Ambulance               406448 non-null  int32  \n",
      " 11  USAGE_Car Hires               406448 non-null  int32  \n",
      " 12  USAGE_Fare Paying Passengers  406448 non-null  int32  \n",
      " 13  USAGE_Fire fighting           406448 non-null  int32  \n",
      " 14  USAGE_General Cartage         406448 non-null  int32  \n",
      " 15  USAGE_Learnes                 406448 non-null  int32  \n",
      " 16  USAGE_Others                  406448 non-null  int32  \n",
      " 17  USAGE_Own Goods               406448 non-null  int32  \n",
      " 18  USAGE_Own service             406448 non-null  int32  \n",
      " 19  USAGE_Private                 406448 non-null  int32  \n",
      " 20  USAGE_Special Construction    406448 non-null  int32  \n",
      " 21  USAGE_Taxi                    406448 non-null  int32  \n",
      "dtypes: float64(2), int32(16), int64(4)\n",
      "memory usage: 43.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_insurance_data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar los registron que tienen PREMIUM null dado que representan un porcentaje pequeño.\n",
    "# Como se mencionó el en desafío 1, según la importancia de la variable se peude reemplazar el valor por la media, mediana\n",
    "# o algun otro valor representativo. \n",
    "df = df_insurance_data_cleaned.dropna(subset=['PREMIUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 406439 entries, 0 to 406447\n",
      "Data columns (total 22 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   CUSTOMER_SENIORITY            406439 non-null  int64  \n",
      " 1   INSURED_VALUE                 406439 non-null  float64\n",
      " 2   PREMIUM                       406439 non-null  float64\n",
      " 3   VEHICLE_ID                    406439 non-null  int64  \n",
      " 4   POLICY_DURATION               406439 non-null  int64  \n",
      " 5   CLAIM_PAID_BINARY             406439 non-null  int64  \n",
      " 6   SEX_Male                      406439 non-null  int32  \n",
      " 7   INSR_TYPE_1202                406439 non-null  int32  \n",
      " 8   INSR_TYPE_1204                406439 non-null  int32  \n",
      " 9   USAGE_Agricultural Own Farm   406439 non-null  int32  \n",
      " 10  USAGE_Ambulance               406439 non-null  int32  \n",
      " 11  USAGE_Car Hires               406439 non-null  int32  \n",
      " 12  USAGE_Fare Paying Passengers  406439 non-null  int32  \n",
      " 13  USAGE_Fire fighting           406439 non-null  int32  \n",
      " 14  USAGE_General Cartage         406439 non-null  int32  \n",
      " 15  USAGE_Learnes                 406439 non-null  int32  \n",
      " 16  USAGE_Others                  406439 non-null  int32  \n",
      " 17  USAGE_Own Goods               406439 non-null  int32  \n",
      " 18  USAGE_Own service             406439 non-null  int32  \n",
      " 19  USAGE_Private                 406439 non-null  int32  \n",
      " 20  USAGE_Special Construction    406439 non-null  int32  \n",
      " 21  USAGE_Taxi                    406439 non-null  int32  \n",
      "dtypes: float64(2), int32(16), int64(4)\n",
      "memory usage: 46.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe destacar también importancia de tratamiento de outliers para evitar distorsiones en el resultado de la prodicción. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la variable objetivo y las características\n",
    "X = df.drop('CLAIM_PAID_BINARY', axis=1)\n",
    "y = df['CLAIM_PAID_BINARY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "# 80/20 es un buen balance porque permite al modelo aprender con suficiente información, sin perder capacidad de generalización, \n",
    "# y garantiza una evaluación robusta con suficientes ejemplos de la clase minoritaria.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para modelos que usa distancia o gradientes es necesario escalar, es decir, \n",
    "# transformar las características numéricas para que tengan un rango o distribución similar\n",
    "# Si se usan áboles de decisión o modelos basados en reglas no es necesario escalar. \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelo\n",
    "\n",
    "Información importante de la base de datos:\n",
    "- Tamaño: El tamaño de conjunto de datos es de 406.439 registros, lo que puede considerarse un tamaño grande. \n",
    "- Relación lineal: \n",
    "- Desbalanceo de clases: Teniendo en cuenta el ratio de desbalance se puede considerar que hay mayor cantidad de pólizas sin reclamo que con reclamo. Se puede considerar que hay un desbalance de clases. \n",
    "\n",
    "Cantidad de pólizas con reclamo (Claim): 33005\n",
    "\n",
    "Cantidad de pólizas sin reclamo (No Claim): 373443\n",
    "\n",
    "Ratio de desbalance: 11.31\n",
    "\n",
    "\n",
    "Es un problema de clasificación de aprendizaje supervisado, entonces los mdoleos qie se pueden utilizar son los siguientes:\n",
    "- Árboles de Decisión\n",
    "- Random Forest\n",
    "- Redes Neronales Artificiales\n",
    "- Gradent Boosting Machines\n",
    "- Support Vector Machine (SVM)\n",
    "- KNN\n",
    "- Naive Bayes\n",
    "- Regresión Logística\n",
    "\n",
    "SVM, KNN, Naive Bayes y Regresión Logística no manejan bien el desbalance de clases de forma nativa. Además, SVM y KNN pueden ser lentos en datasets grandes debido a sus métodos computacionales. Naive Bayes asume independencia condicional entre variables, lo que puede generar problemas si existe una fuerte correlación entre atributos (ej. PREMIUM e INSURED_VALUE). Regresión Logística, al ser un modelo lineal, no captura relaciones complejas entre variables y el objetivo, lo que puede limitar su capacidad predictiva en este problema.\n",
    "\n",
    "¿Cómo saber qué métrica es importante para este problema? \n",
    "La métrica más importante depende del objetivo del negocio. En este caso, se está prediciendo si una póliza de seguro hará un reclamo o no. Como la clase positiva (CLAIM_PAID = 1) es minoritaria (~8%), se pued econsiderar:\n",
    "- Accuracy (Precisión global del modelo)\n",
    "    - Útil si se quiere que el modelo sea bueno en todas las clases.\n",
    "    - Funciona bien cuando las clases están balanceadas. Pero como la clase 0 (CLAIM_PAID = 0) es el 92% de los datos, un modelo puede alcanzar un 92% de accuracy simplemente prediciendo que nadie hará un reclamo.\n",
    "Conclusión: NO es la mejor métrica para este caso.\n",
    "- Recall en la clase 1 (Reclamos detectados correctamente)\n",
    "    - Útil cuando los falsos negativos (pólizas que deberían hacer un reclamo pero el modelo dice que no) son costosos o problemáticos. \n",
    "    - Si la aseguradora quiere identificar proactivamente clientes con alto riesgo de reclamo y ajustar precios o políticas.\n",
    "    - Puede suceder que si maximizamos el recall al 100%, podríamos predecir que todas las pólizas harán un reclamo, lo que generaría muchos falsos positivos.\n",
    "    \n",
    "Conclusión: \n",
    "- Priorizar Recall sobre Accuracy porque se quiere detectar la mayor cantidad de reclamos posibles.\n",
    "- F1-score es mejor que Accuracy porque equilibra precisión y recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes - Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7223329396712922\n",
      "Matriz de confusión:\n",
      " [[55376 19374]\n",
      " [ 3197  3341]]\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83     74750\n",
      "           1       0.15      0.51      0.23      6538\n",
      "\n",
      "    accuracy                           0.72     81288\n",
      "   macro avg       0.55      0.63      0.53     81288\n",
      "weighted avg       0.88      0.72      0.78     81288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y evaluar MultinomialNB (para datos discretos como conteos)\n",
    "mnb_clf = MultinomialNB()\n",
    "\n",
    "# Entrenar el modelo\n",
    "mnb_clf.fit(X_train, y_train)\n",
    "y_pred = mnb_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó menos de 1 segundo. \n",
    "\n",
    "Parámetros: {'alpha': 1.0}\n",
    "\n",
    "Accuracy: 0.7223329396712922\n",
    "\n",
    "Matriz de confusión:\n",
    "\n",
    "    55376 19374\n",
    "    3197  3341\n",
    "\n",
    "Reporte de clasificación:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.74      0.83     74750\n",
    "           1       0.15      0.51      0.23      6538\n",
    "\n",
    "    accuracy                           0.72     81288\n",
    "    macro avg       0.55      0.63      0.53     81288\n",
    "    weighted avg    0.88      0.72      0.78     81288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'alpha': 0.01}\n",
      "Mejor AUC: 0.7209327331705413\n",
      "Accuracy del modelo optimizado: 0.7223329396712922\n",
      "Matriz de confusión del modelo optimizado:\n",
      " [[55376 19374]\n",
      " [ 3197  3341]]\n",
      "Reporte de clasificación del modelo optimizado:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83     74750\n",
      "           1       0.15      0.51      0.23      6538\n",
      "\n",
      "    accuracy                           0.72     81288\n",
      "   macro avg       0.55      0.63      0.53     81288\n",
      "weighted avg       0.88      0.72      0.78     81288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GridSearch para MultinomialNB\n",
    "param_grid_mnb = {'alpha': [0.01, 0.1, 1, 10]}\n",
    "grid_mnb = GridSearchCV(MultinomialNB(), param_grid_mnb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_mnb.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_grid_mnb = grid_mnb.best_estimator_\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_mnb.best_params_)\n",
    "print(\"Mejor AUC:\", grid_mnb.best_score_)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = best_grid_mnb.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"Accuracy del modelo optimizado:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Matriz de confusión del modelo optimizado:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Reporte de clasificación del modelo optimizado:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 2 segundos.\n",
    "\n",
    "Mejores parámetros: {'alpha': 0.01}\n",
    "\n",
    "Accuracy del modelo optimizado: 0.7223329396712922\n",
    "\n",
    "Matriz de confusión del modelo optimizado:\n",
    "\n",
    "    55376 19374\n",
    "    3197  3341\n",
    "\n",
    "Reporte de clasificación del modelo optimizado:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.74      0.83     74750\n",
    "           1       0.15      0.51      0.23      6538\n",
    "\n",
    "    accuracy                           0.72     81288\n",
    "    macro avg       0.55      0.63      0.53     81288\n",
    "    weighted avg    0.88      0.72      0.78     81288\n",
    "\n",
    "La optimziación no generó una diferencia significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy modelo Random Forest: 0.9095807499261883\n",
      "Confusion Matrix modelo Random Forest:\n",
      "[[73467  1283]\n",
      " [ 6067   471]]\n",
      "Classification Report modelo Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95     74750\n",
      "           1       0.27      0.07      0.11      6538\n",
      "\n",
      "    accuracy                           0.91     81288\n",
      "   macro avg       0.60      0.53      0.53     81288\n",
      "weighted avg       0.87      0.91      0.88     81288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el clasificador\n",
    "random_forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "random_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(f'Accuracy modelo Random Forest: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix modelo Random Forest:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report modelo Random Forest:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
    "\n",
    "Tardó 2 minutos.\n",
    "\n",
    "Accuracy modelo Random Forest: 0.91\n",
    "\n",
    "Confusion Matrix modelo Random Forest:\n",
    "\n",
    "      73467  1283\n",
    "      6067   471\n",
    "\n",
    "Classification Report modelo Random Forest:\n",
    "              \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.98      0.95     74750\n",
    "           1       0.27      0.07      0.11      6538\n",
    "\n",
    "    accuracy                           0.91     81288\n",
    "    macro avg       0.60      0.53      0.53     81288\n",
    "    weighted avg    0.87      0.91      0.88     81288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Mejores parámetros: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 15}\n",
      "Mejor AUC: 0.918573216816272\n",
      "Accuracy del modelo optimizado: 0.9197544533018404\n",
      "Matriz de confusión del modelo optimizado:\n",
      " [[74724    26]\n",
      " [ 6497    41]]\n",
      "Reporte de clasificación del modelo optimizado:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     74750\n",
      "           1       0.61      0.01      0.01      6538\n",
      "\n",
      "    accuracy                           0.92     81288\n",
      "   macro avg       0.77      0.50      0.49     81288\n",
      "weighted avg       0.90      0.92      0.88     81288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uso de RandomizedSearchCV en lugar de GridSearchCV y se demore menos (aunque no garantiza encontrar el mejor posible).\n",
    "# Elije subconjunto aleatorio de hiperparámetros en lugar de probar todas las combinaciones.\n",
    "\n",
    "# Definir el espacio de búsqueda optimizado (menos combinaciones que GridSearch)\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100],  \n",
    "    'max_depth': [10, 15], \n",
    "    'min_samples_split': [2, 5],  \n",
    "    'min_samples_leaf': [1, 2],  \n",
    "    'max_features': ['sqrt'] \n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV en lugar de GridSearchCV\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  \n",
    "    cv=3,  # Menos validaciones cruzadas para reducir el tiempo\n",
    "    n_jobs=-1,  # Usa todos los núcleos disponibles\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_rf_clf = random_search_rf.best_estimator_\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Mejores parámetros:\", random_search_rf.best_params_)\n",
    "print(\"Mejor AUC:\", random_search_rf.best_score_)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"Accuracy del modelo optimizado:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Matriz de confusión del modelo optimizado:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Reporte de clasificación del modelo optimizado:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 4 minutos.\n",
    "\n",
    "Mejores parámetros: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 15}\n",
    "\n",
    "Mejor AUC: 0.918573216816272\n",
    "\n",
    "Accuracy del modelo optimizado: 0.9197544533018404\n",
    "\n",
    "Matriz de confusión del modelo optimizado:\n",
    "\n",
    "    74724    26\n",
    "    6497    41\n",
    "\n",
    "Reporte de clasificación del modelo optimizado:\n",
    "               \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.61      0.01      0.01      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.77      0.50      0.49     81288\n",
    "    weighted avg    0.90      0.92      0.88     81288\n",
    "\n",
    "Mejoró la presición, pero bajó el recall y el F1-score de la clas 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy modelo XGBoost: 0.919902076567267\n",
      "Confusion Matrix modelo XGBoost:\n",
      "[[74700    50]\n",
      " [ 6461    77]]\n",
      "Classification Report modelo XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     74750\n",
      "           1       0.61      0.01      0.02      6538\n",
      "\n",
      "    accuracy                           0.92     81288\n",
      "   macro avg       0.76      0.51      0.49     81288\n",
      "weighted avg       0.90      0.92      0.88     81288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelo de XGBoost para clasificación\n",
    "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "\n",
    "# Entrenar el modelo\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(f'Accuracy modelo XGBoost: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Confusion Matrix modelo XGBoost:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "print(f'Classification Report modelo XGBoost:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 2 segundos.\n",
    "\n",
    "Parámetros: {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.3, 'colsample_bytree': 1.0}\n",
    "\n",
    "Accuracy modelo XGBoost: 0.919902076567267\n",
    "\n",
    "Confusion Matrix modelo XGBoost:\n",
    "\n",
    "    74700    50\n",
    "    6461    77\n",
    "\n",
    "Classification Report modelo XGBoost:\n",
    "              \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.61      0.01      0.02      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.76      0.51      0.49     81288\n",
    "    weighted avg    0.90      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Mejores parámetros: {'subsample': 0.7, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
      "Mejor AUC: 0.9186654816125485\n",
      "Accuracy del modelo optimizado: 0.9198036610569826\n",
      "Matriz de confusión del modelo optimizado:\n",
      " [[74724    26]\n",
      " [ 6493    45]]\n",
      "Reporte de clasificación del modelo optimizado:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     74750\n",
      "           1       0.63      0.01      0.01      6538\n",
      "\n",
      "    accuracy                           0.92     81288\n",
      "   macro avg       0.78      0.50      0.49     81288\n",
      "weighted avg       0.90      0.92      0.88     81288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir el espacio de búsqueda optimizado\n",
    "param_dist_xgb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'subsample': [0.7, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(),\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=20,  # Número de combinaciones a probar \n",
    "    cv=3,  # Número de folds, si hay mayor tiempo se peude aumentar. \n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Usa todos los núcleos disponibles\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Mejores parámetros:\", random_search_xgb.best_params_)\n",
    "print(\"Mejor AUC:\", random_search_xgb.best_score_)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"Accuracy del modelo optimizado:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Matriz de confusión del modelo optimizado:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Reporte de clasificación del modelo optimizado:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 2 minutos. \n",
    "\n",
    "Mejores parámetros: {'subsample': 0.7, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
    "\n",
    "Mejor AUC: 0.9186654816125485\n",
    "\n",
    "Accuracy del modelo optimizado: 0.9198036610569826\n",
    "\n",
    "Matriz de confusión del modelo optimizado:\n",
    "\n",
    "    74724    26\n",
    "    6493    45\n",
    "\n",
    "Reporte de clasificación del modelo optimizado:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.63      0.01      0.01      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.78      0.50      0.49     81288\n",
    "    weighted avg    0.90      0.92      0.88     81288\n",
    "\n",
    "No se presenta una diferencia significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo\n",
    "\n",
    "Conclusiones principales:\n",
    "- Si lo más importante es detectar la mayor cantidad de reclamos (recall alto) → Naïve Bayes (Multinomial) con 0.51 de recall.\n",
    "- Si se busca un balance entre recall y precisión (mejor F1-score) → Naïve Bayes (Multinomial) con 0.23 de F1-score.\n",
    "- Si se busca reducir falsos positivos pero mantener un buen recall → Random Forest Optimizado o Gradient Boosting Machines.\n",
    "\n",
    "Se tendrá en cuenta la primera conclusión para predecir el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mnb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo test.csv y preprocesarlo de la misma manera\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101988 entries, 0 to 101987\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   POLICY_ID           101988 non-null  int64  \n",
      " 1   INSR_BEGIN          101988 non-null  object \n",
      " 2   INSR_END            101988 non-null  object \n",
      " 3   CUSTOMER_SENIORITY  101988 non-null  int64  \n",
      " 4   SEX                 101988 non-null  object \n",
      " 5   INSR_TYPE           101988 non-null  int64  \n",
      " 6   INSURED_VALUE       101988 non-null  float64\n",
      " 7   PREMIUM             101984 non-null  float64\n",
      " 8   VEHICLE_ID          101988 non-null  int64  \n",
      " 9   USAGE               101988 non-null  object \n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 7.8+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el mismo preprocesamiento que en el conjunto de entrenamiento\n",
    "# (mismo tratamiento de fechas, codificación y escalado)\n",
    "\n",
    "# Rango de fecha\n",
    "test_df['INSR_BEGIN'] = pd.to_datetime(test_df['INSR_BEGIN'], format='%d-%b-%y')\n",
    "test_df['INSR_END'] = pd.to_datetime(test_df['INSR_END'], format='%d-%b-%y')\n",
    "\n",
    "# Crear la columna de duración de la póliza en días\n",
    "test_df['POLICY_DURATION'] = (test_df['INSR_END'] - test_df['INSR_BEGIN']).dt.days\n",
    "\n",
    "# Eliminar los registron que tienen PREMIUM null dado que representan un porcentaje pequeño.\n",
    "test_df = test_df.dropna(subset=['PREMIUM'])\n",
    "\n",
    "# Eliminar las columnas que no voy a usar para el modelo. Claim_Paid, fechas, policy_id\n",
    "test_df = test_df.drop(columns=['INSR_BEGIN', 'INSR_END', 'POLICY_ID'])\n",
    "\n",
    "# Codificación one-hot para columnas categóricas\n",
    "test_df = pd.get_dummies(test_df, columns=['SEX', 'INSR_TYPE', 'USAGE'], drop_first=True, dtype = int)\n",
    "\n",
    "# test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir las pólizas que tendrán un reclamo\n",
    "y_pred_test = model.predict(test_df)\n",
    "\n",
    "# Guardar las predicciones\n",
    "test_df['CLAIM_PREDICTION'] = y_pred_test\n",
    "test_df.to_csv('Predicciones.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Próximos pasos\n",
    "\n",
    "Luego de tener un modelo armado se recomienda tener en cuenta lo siguiente:\n",
    "- Tener en cuenta los tiempos de ejecución y la frecuencia de entrenamiento del modelo, esto último puede depender de los cambios y actualizaciones en la input database.\n",
    "- Optimizar la utilización de hiperparámetros, recoletar más datos para evitar el desbalanceo, y/o eliminar variables que puedne ser redundantes (por ejemplo si INSURED_VALUE correlación con PREMIUM).\n",
    "- Implementarlo un entorno de producción (por ejemplo Databricks si se necesita una mayor capacidad de procesamiento aprovechando la utilización de clusters). \n",
    "- Realizar la documentación del modelo.\n",
    "- Monitorear el desempeño y evaluando el rendiminento del modelo en tiempo real para desarrollar estrategias que impulsen a favor del negocio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear un modelo de árbol de decisión sin optimización\n",
    "# tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predicciones\n",
    "# y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(\"Accuracy modelo Árbol de Decisión:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión modelo Árbol de Decisión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación modelo Árbol de Decisión:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 3 segundos. \n",
    "\n",
    "Parámetros por default: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "\n",
    "Accuracy modelo Árbol de Decisión: 0.87\n",
    "\n",
    "Matriz de confusión modelo Árbol de Decisión:\n",
    "\n",
    "        69436  5314\n",
    "        5384  1154\n",
    "\n",
    "Reporte de clasificación modelo Árbol de Decisión:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "           0       0.93      0.93      0.93     74750\n",
    "           1       0.18      0.18      0.18      6538\n",
    "\n",
    "    accuracy                           0.87     81288\n",
    "    macro avg      0.55      0.55      0.55     81288\n",
    "    weighted avg   0.87      0.87      0.87     81288\n",
    "\n",
    "La precisión para la clase mayoritaria (0) es muy alta (0.93), mientras que para la clase minoritaria (1) es extremadamente baja (0.18). El recall también muestra que el modelo tiene problemas para identificar correctamente las instancias de la clase minoritaria (0.18); lo que indica un sesgo hacia la predicción de la clase mayoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# plot_tree(tree_clf, filled=True, feature_names=X.columns, class_names=['No Claim', 'Claim'], rounded=True)\n",
    "# plt.title(\"Árbol de Decisión - Modelo Sin Optimizar\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir los parámetros a optimizar\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],  # Criterio de división\n",
    "#     'max_depth': [3, 5, 10, None],  # Profundidad máxima\n",
    "#     'min_samples_split': [2, 5, 10],  # Mínimo de muestras para dividir un nodo\n",
    "#     'min_samples_leaf': [1, 2, 4],  # Mínimo de muestras en una hoja\n",
    "#     'max_features': ['sqrt', 'log2', None]  # Cantidad de features a considerar en cada split\n",
    "# }\n",
    "\n",
    "# # Realizar la búsqueda de hiperparámetros con validación cruzada\n",
    "# grid_search_tree_clf = GridSearchCV(\n",
    "#     DecisionTreeClassifier(random_state=42), \n",
    "#     param_grid, \n",
    "#     cv=5,  # Número de folds, si hay mayor tiempo se peude aumentar. \n",
    "#     scoring='recall', \n",
    "#     n_jobs=-1  # Usa todos los núcleos disponibles\n",
    "# )\n",
    "# grid_search_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_tree_clf = grid_search_tree_clf.best_estimator_\n",
    "\n",
    "# print(\"Mejores parámetros:\", grid_search_tree_clf.best_params_)\n",
    "# print(\"Mejor AUC:\", grid_search_tree_clf.best_score_)\n",
    "\n",
    "# # Predicciones\n",
    "# y_pred = best_tree_clf.predict(X_test)\n",
    "\n",
    "# # Evaluar el modelo optimizado\n",
    "# print(\"Accuracy del modelo optimizado Árbol de Decisión:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión del modelo optimizado Árbol de Decisión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación del modelo optimizado Árbol de Decisión:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 5 minutos. \n",
    "\n",
    "Mejores parámetros: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "\n",
    "Mejor AUC: 0.17349600111050603\n",
    "\n",
    "Accuracy del modelo optimizado Árbol de Decisión: 0.8683938588721583\n",
    "\n",
    "Matriz de confusión del modelo optimizado Árbol de Decisión:\n",
    "\n",
    "        69436  5314\n",
    "        5384  1154\n",
    "\n",
    "Reporte de clasificación del modelo optimizado Árbol de Decisión:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "           0       0.93      0.93      0.93     74750\n",
    "           1       0.18      0.18      0.18      6538\n",
    "\n",
    "    accuracy                           0.87     81288\n",
    "    macro avg       0.55      0.55      0.55     81288\n",
    "    weighted avg    0.87      0.87      0.87     81288\n",
    "\n",
    "El modelo optimziado tiene los mismos parámetros que por default del modelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graficar el árbol de decisión optimizado\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plot_tree(best_tree_clf, filled=True, feature_names=X.columns, class_names=['No Claim', 'Claim'], rounded=True)\n",
    "# plt.title(\"Árbol de Decisión - Modelo Optimizado\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_classifier = SVC(kernel='linear')\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Make Prediction & print the result\n",
    "# y_pred = svc_classifier.predict(X_test)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "# print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tardando mucho (más de 16 minutos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_sigmoid_clf = SVC(kernel='sigmoid')\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# svc_sigmoid_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Make Prediction & print the result\n",
    "# y_pred = svc_sigmoid_clf.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(f'Accuracy modelo SVM Sigmoid: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'Confusion Matrix modelo SVM Sigmoid:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "# print(f'Classification Report modelo SVM Sigmoid:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 40 minutos. \n",
    "\n",
    "Accuracy: 0.9195699242200571\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "    74750     0\n",
    "    6538     0\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.00      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.46      0.50      0.48     81288\n",
    "    weighted avg    0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polinómico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_poly_clf = SVC(kernel='poly')\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# svc_poly_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Make Prediction & print the result\n",
    "# y_pred = svc_poly_clf.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(f'Accuracy modelo SVM Poly: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'Confusion Matrix modelo SVM Poly:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "# print(f'Classification Report modelo SVM Poly:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 14 minutos.\n",
    "\n",
    "Accuracy: 0.9195699242200571\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "    74750     0\n",
    "    6538     0\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.00      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.46      0.50      0.48     81288\n",
    "    weighted avg       0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Radial Basis Function (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_rbf_clf = SVC(kernel='rbf')\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# svc_rbf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Make Prediction & print the result\n",
    "# y_pred = svc_rbf_clf.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(f'Accuracy modelo SVM RBF: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'Confusion Matrix modelo SVM RBF:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "# print(f'Classification Report modelo SVM RBF:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 30 minutos. \n",
    "\n",
    "Accuracy: 0.9195699242200571\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "    74750     0\n",
    "    6538     0\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.00      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.46      0.50      0.48     81288\n",
    "    weighted avg    0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear el modelo KNN con k=3\n",
    "# knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# knn_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Hacer predicciones\n",
    "# y_pred = knn_clf.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(f'Accuracy modelo KNN: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'Confusion Matrix modelo KNN:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "# print(f'Classification Report modelo KNN:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó menos de 2 minutos.\n",
    "\n",
    "Accuracy: 0.8996407833874619\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "    108720   3357\n",
    "    8880    975\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.97      0.95    112077\n",
    "           1       0.23      0.10      0.14      9855\n",
    "\n",
    "    accuracy                           0.90    121932\n",
    "    macro avg       0.57      0.53      0.54    121932\n",
    "    weighted avg    0.87      0.90      0.88    121932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenar y evaluar GaussianNB (para datos continuos)\n",
    "# gnb_clf = GaussianNB()\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# gnb_clf.fit(X_train, y_train)\n",
    "# y_pred = gnb_clf.predict(X_test)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó menos de 1 segundo.\n",
    "\n",
    "Accuracy: 0.9171341403405177\n",
    "\n",
    "Matriz de confusión:\n",
    "\n",
    "    74545   205\n",
    "    6531     7\n",
    "\n",
    "Reporte de clasificación:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.03      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.48      0.50      0.48     81288\n",
    "    weighted avg    0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GridSearch para GaussianNB\n",
    "# param_grid_gnb = {'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "# grid_gnb = GridSearchCV(GaussianNB(), param_grid_gnb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Ajustar el modelo a los datos de entrenamiento\n",
    "# grid_gnb.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_grid_gnb = grid_gnb.best_estimator_\n",
    "\n",
    "# print(\"Mejores parámetros:\", grid_gnb.best_params_)\n",
    "# print(\"Mejor AUC:\", grid_gnb.best_score_)\n",
    "\n",
    "# # Predicciones\n",
    "# y_pred = best_grid_gnb.predict(X_test)\n",
    "\n",
    "# # Evaluar el modelo optimizado\n",
    "# print(\"Accuracy del modelo optimizado:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión del modelo optimizado:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación del modelo optimizado:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 6 segundos.\n",
    "\n",
    "Accuracy del modelo optimizado: 0.9179460683003642\n",
    "\n",
    "Matriz de confusión del modelo optimizado:\n",
    "\n",
    "    74616   134\n",
    "    6536     2\n",
    "\n",
    "Reporte de clasificación del modelo optimizado:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.01      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.47      0.50      0.48     81288\n",
    "    weighted avg    0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bernoulli Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenar y evaluar BernoulliNB (para datos binarios)\n",
    "# bnb_clf = BernoulliNB()\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# bnb_clf.fit(X_train, y_train)\n",
    "# y_pred = bnb_clf.predict(X_test)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó menos de 1 segundo.\n",
    "\n",
    "Accuracy: 0.9195699242200571\n",
    "\n",
    "Matriz de confusión:\n",
    "\n",
    "    74750     0\n",
    "    6538     0\n",
    "\n",
    "Reporte de clasificación:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.00      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.46      0.50      0.48     81288\n",
    "    weighted avg    0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GridSearch para BernoulliNB\n",
    "# param_grid_bnb = {'alpha': [0.01, 0.1, 1, 10], 'binarize': [0.0, 0.5, 1.0]}\n",
    "# grid_bnb = GridSearchCV(BernoulliNB(), param_grid_bnb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Ajustar el modelo a los datos de entrenamiento\n",
    "# grid_bnb.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_grid_bnb = grid_bnb.best_estimator_\n",
    "\n",
    "# print(\"Mejores parámetros:\", grid_bnb.best_params_)\n",
    "# print(\"Mejor AUC:\", grid_bnb.best_score_)\n",
    "\n",
    "# # Predicciones\n",
    "# y_pred = best_grid_bnb.predict(X_test)\n",
    "\n",
    "# # Evaluar el modelo optimizado\n",
    "# print(\"Accuracy del modelo optimizado:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión del modelo optimizado:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación del modelo optimizado:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 9 segundos.\n",
    "\n",
    "Accuracy del modelo optimizado: 0.9195699242200571\n",
    "\n",
    "Matriz de confusión del modelo optimizado:\n",
    "\n",
    "    74750     0\n",
    "    6538     0\n",
    "\n",
    "Reporte de clasificación del modelo optimizado:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.00      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.46      0.50      0.48     81288\n",
    "    weighted avg    0.85      0.92      0.88     81288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelo de regresión logística\n",
    "# logreg_clf = LogisticRegression()\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# logreg_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Predicciones\n",
    "# y_pred = logreg_clf.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# print(f'Accuracy modelo Regresión Logística: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'Confusion Matrix modelo Regresión Logística:\\n{confusion_matrix(y_test, y_pred)}')\n",
    "# print(f'Classification Report modelo Regresión Logística:\\n{classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 3 segundos. \n",
    "\n",
    "Accuracy: 0.9183520322802874\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "    74636   114\n",
    "    6523    15\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.12      0.00      0.00      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.52      0.50      0.48     81288\n",
    "    weighted avg    0.86      0.92      0.88     81288\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal Artificial (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construir la red neuronal\n",
    "# ann_clf = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dropout(0.3),  # Evita overfitting\n",
    "#     Dense(1, activation='sigmoid')  # Salida binaria (clasificación)\n",
    "# ])\n",
    "\n",
    "# # Compilar el modelo\n",
    "# ann_clf.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# history = ann_clf.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# y_pred = (ann_clf.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# print(\"Accuracy modelo ANN:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de confusión del modelo optimizado modelo ANN:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de clasificación del modelo optimizado modelo ANN:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tardó 33 minutos.\n",
    "\n",
    "Accuracy: 0.9197421513630548\n",
    "\n",
    "Matriz de confusión del modelo optimizado:\n",
    "\n",
    "    74732    18\n",
    "    6506    32\n",
    "\n",
    "Reporte de clasificación del modelo optimizado:\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     74750\n",
    "           1       0.64      0.00      0.01      6538\n",
    "\n",
    "    accuracy                           0.92     81288\n",
    "    macro avg       0.78      0.50      0.48     81288\n",
    "    weighted avg    0.90      0.92      0.88     81288"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
